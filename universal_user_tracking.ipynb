{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tracking individual users\n",
    "This is the jupyter version of python script that will be used to track individual users jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing epmt_query\n",
      "importing pandas\n",
      "dictionary saved successfully to file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import  epmt query \n",
    "print('importing epmt_query')\n",
    "import epmt_query as eq\n",
    "# import matplot for better plotting functions\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import os.path\n",
    "# import pandas. optional but helpful 'display.max_columns' arg shows all DataFrame columns when printing\n",
    "print('importing pandas')\n",
    "import pandas as pd\n",
    "import pickle   #to load in metrics history\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#We gather initial estimates and get a list of IDs for the specific date\n",
    "today_date = datetime.datetime.today()\n",
    "start_date = today_date + datetime.timedelta(days =-7)   #day that ends the week\n",
    "older_date = start_date + datetime.timedelta(days =-1) \n",
    "date = (start_date.strftime(\"%m\")+'-'+start_date.strftime(\"%d\")+'-'+start_date.strftime(\"%y\")) #date in an easy to read format\n",
    "#use orm and trigger_post_process=False to get data quickly\n",
    "older_orm = eq.get_jobs(after=older_date, fmt = 'orm',trigger_post_process=False)\n",
    "start_orm = eq.get_jobs(after=start_date, fmt = 'orm',trigger_post_process=False)\n",
    "job_num = older_orm.count() - start_orm.count()    #clip off the jobs that happened between current time and the end of the week being reported on\n",
    "all_jobs=eq.get_jobs(limit =int(job_num), offset = start_orm.count(), fmt='orm',trigger_post_process=False)\n",
    "\n",
    "names = ['Ian.Laflotte', 'Raphael.Dussin','William.Cooke','Lori.Sentman','Huan.Guo','Olga.Sergienko','Mitchell.Bushuk','Linjiong.Zhou','Larry.Horowitz','Colleen.McHugh ','Hiroyuki.Murakami','Ming.Zhao']\n",
    "for name_number in range(len(names)):\n",
    "    filename = 'user_hist_storage_folder/'+names[name_number]+'_history_'+'DO_NOT_DELETE.pkl'\n",
    "\n",
    "    #check if history file exists, and create if not\n",
    "    if os.path.isfile(filename) == False:\n",
    "        #Be careful, as this creates a blank file for the dictionary of metrics. If used foolishly, can delete long term data. Only use if creating new folder or want to reset all data\n",
    "        #creat dictionarymetrics_dict['canopy']\n",
    "        metrics_list = {'rssmax','write_bytes','duration','cpu_time','read_bytes', 'num_procs', 'time_waiting'}\n",
    "        metrics_dict = {}\n",
    "        for metric in metrics_list:\n",
    "            metrics_dict[metric] = []\n",
    "            metrics_dict[metric+'_error'] = []\n",
    "        metrics_dict['flux'] = []\n",
    "        metrics_dict['flux_error'] = []\n",
    "        metrics_dict['num_jobs'] = []\n",
    "        metrics_dict['date'] = []\n",
    "\n",
    "        # save dictionary to weekly_metric_storage_DO_NOT_DELETE.pkl file\n",
    "        with open(filename, 'wb') as fp:\n",
    "            pickle.dump(metrics_dict, fp)\n",
    "            print('new dictionary saved successfully to file')\n",
    "\n",
    "    \n",
    "    print(names[name_number])\n",
    "    #sort data by esm4.2 tag in user_name\n",
    "    user_jobids=[]\n",
    "    total_jobs=all_jobs.count()\n",
    "    job_num=0\n",
    "    for job in all_jobs:\n",
    "        job_num=job_num+1\n",
    "        user_name=str(job.user)\n",
    "        if user_name is None:\n",
    "            continue\n",
    "        if (names[name_number] in user_name) or (names[name_number].lower() in user_name.lower()):\n",
    "            user_jobids.append(job.jobid) \n",
    "\n",
    "    if len(user_jobids) < 1:\n",
    "        print('insuficient data')\n",
    "    else:    \n",
    "\n",
    "\n",
    "        #get jobs in dict format, if there is more than 10\n",
    "        if len(user_jobids) >= 1:\n",
    "            #setup dictionaries\n",
    "            metrics_list = {'rssmax','write_bytes','duration','cpu_time','read_bytes', 'num_procs', 'time_waiting'}\n",
    "            metrics_dict = {}\n",
    "            for metric in metrics_list:\n",
    "                metrics_dict[metric] = []\n",
    "            metrics_dict['flux'] = []\n",
    "            esm4p2_jobs = eq.get_jobs(jobs = user_jobids, fmt = 'dict', trigger_post_process=False)\n",
    "            for job in esm4p2_jobs:\n",
    "                if job.get('rssmax') == None:\n",
    "                    job = eq.get_jobs(jobs = job['jobid'], fmt = 'dict')[0]\n",
    "\n",
    "                for metric in metrics_list:\n",
    "                    metrics_dict[metric].append(job[metric])\n",
    "                metrics_dict['flux'].append((job['read_bytes']+job['write_bytes'])/job['cpu_time'])\n",
    "\n",
    "        #process data by getting average, and add errors\n",
    "        for metric in list(metrics_dict.keys()):\n",
    "            metrics_dict[metric+'_error'] = np.std(metrics_dict[metric])/(len(metrics_dict[metric]))**.5\n",
    "            metrics_dict[metric] = sum(metrics_dict[metric])/len(metrics_dict[metric])\n",
    "        metrics_dict['date'] = date\n",
    "        metrics_dict['num_jobs'] = len(user_jobids)\n",
    "\n",
    "        #record metrics based on date (to avoid repitition)\n",
    "        # Read dictionary pkl file\n",
    "        with open(filename, 'rb') as fp:\n",
    "            metrics_history = pickle.load(fp)\n",
    "        #record new dates\n",
    "        if date not in metrics_history['date']:\n",
    "            for key in list(metrics_history.keys()):\n",
    "                metrics_history[key].append(metrics_dict[key])\n",
    "        # save dictionary to weekly_metric_storage_DO_NOT_DELETE.pkl file\n",
    "        with open(filename, 'wb') as fp:\n",
    "            pickle.dump(metrics_history, fp)\n",
    "            print('dictionary saved successfully to file')\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
